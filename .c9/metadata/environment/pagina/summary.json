{"filter":false,"title":"summary.json","tooltip":"/pagina/summary.json","undoManager":{"mark":59,"position":59,"stack":[[{"start":{"row":0,"column":0},"end":{"row":12,"column":0},"action":"insert","lines":["{","  \"summary\": \"El modelo local supera al de HuggingFace en recall_macro (+0.012) y empata en accuracy; HuggingFace conserva ligera ventaja en precision_macro.\",","  \"table\": {","    \"headers\": [\"Métrica\", \"Tu Modelo\", \"HuggingFace\", \"Diferencia\", \"Conjunto\"],","    \"rows\": [","      [\"accuracy\", \"0.9043\", \"0.9041\", \"+0.0002\", \"Test\"],","      [\"precision_macro\", \"0.8920\", \"0.8975\", \"-0.0055\", \"Test\"],","      [\"recall_macro\", \"0.9012\", \"0.8892\", \"+0.0120\", \"Test\"],","      [\"f1_macro\", \"0.8966\", \"0.8933\", \"+0.0033\", \"Test\"]","    ]","  }","}",""],"id":62}],[{"start":{"row":0,"column":0},"end":{"row":12,"column":0},"action":"remove","lines":["{","  \"summary\": \"El modelo local supera al de HuggingFace en recall_macro (+0.012) y empata en accuracy; HuggingFace conserva ligera ventaja en precision_macro.\",","  \"table\": {","    \"headers\": [\"Métrica\", \"Tu Modelo\", \"HuggingFace\", \"Diferencia\", \"Conjunto\"],","    \"rows\": [","      [\"accuracy\", \"0.9043\", \"0.9041\", \"+0.0002\", \"Test\"],","      [\"precision_macro\", \"0.8920\", \"0.8975\", \"-0.0055\", \"Test\"],","      [\"recall_macro\", \"0.9012\", \"0.8892\", \"+0.0120\", \"Test\"],","      [\"f1_macro\", \"0.8966\", \"0.8933\", \"+0.0033\", \"Test\"]","    ]","  }","}",""],"id":63},{"start":{"row":0,"column":0},"end":{"row":13,"column":0},"action":"insert","lines":["{","  \"text_top\": \"Resumen breve antes de la tabla: contexto y lectura recomendada.\",","  \"table\": {","    \"headers\": [\"Métrica\", \"Tu Modelo\", \"HuggingFace\", \"Diferencia\", \"Conjunto\"],","    \"rows\": [","      [\"accuracy\", \"0.9043\", \"0.9041\", \"+0.0002\", \"Test\"],","      [\"precision_macro\", \"0.8920\", \"0.8975\", \"-0.0055\", \"Test\"],","      [\"recall_macro\", \"0.9012\", \"0.8892\", \"+0.0120\", \"Test\"],","      [\"f1_macro\", \"0.8966\", \"0.8933\", \"+0.0033\", \"Test\"]","    ]","  },","  \"text_bottom\": \"Comentario final después de la tabla: conclusiones y próximos pasos.\"","}",""]}],[{"start":{"row":1,"column":15},"end":{"row":1,"column":79},"action":"remove","lines":["Resumen breve antes de la tabla: contexto y lectura recomendada."],"id":64},{"start":{"row":1,"column":15},"end":{"row":3,"column":106},"action":"insert","lines":["Cuando se realiza la comparativa de Modelos de Análisis de Sentimientos se puede ver que en este caso, el objetivo fue desarrollar un modelo de clasificación de sentimientos para datos de Twitter y comparar su rendimiento con el State-Of-The-Art, como el pipeline sentiment-analysis de Hugging Face, que utiliza el modelo DistilBERT fine-tuned en SST-2. El mejor modelo desarrollado se determina que fue un Random Forest (RF), evaluado principalmente con la métrica de recall debido al caso de uso planteado y acotacion del problema, siendo este el filtrar contenido con connotación negativa para audiencias sensibles, donde es crucial minimizar los falsos negativos. Originalmente planteado como el filtro para un público infantil y evitar su exposición a emociones negativas.","","Al evaluar ambos modelos en sus respectivos conjuntos de prueba, se obtuvieron los siguientes resultados:\t"]}],[{"start":{"row":1,"column":118},"end":{"row":2,"column":0},"action":"insert","lines":["",""],"id":65},{"start":{"row":2,"column":0},"end":{"row":2,"column":2},"action":"insert","lines":["  "]}],[{"start":{"row":2,"column":146},"end":{"row":3,"column":0},"action":"insert","lines":["",""],"id":66},{"start":{"row":3,"column":0},"end":{"row":3,"column":2},"action":"insert","lines":["  "]}],[{"start":{"row":3,"column":141},"end":{"row":4,"column":0},"action":"insert","lines":["",""],"id":67},{"start":{"row":4,"column":0},"end":{"row":4,"column":2},"action":"insert","lines":["  "]}],[{"start":{"row":4,"column":150},"end":{"row":5,"column":0},"action":"insert","lines":["",""],"id":68},{"start":{"row":5,"column":0},"end":{"row":5,"column":2},"action":"insert","lines":["  "]}],[{"start":{"row":5,"column":150},"end":{"row":6,"column":0},"action":"insert","lines":["",""],"id":69},{"start":{"row":6,"column":0},"end":{"row":6,"column":2},"action":"insert","lines":["  "]}],[{"start":{"row":7,"column":0},"end":{"row":8,"column":0},"action":"remove","lines":["",""],"id":70}],[{"start":{"row":7,"column":0},"end":{"row":7,"column":4},"action":"insert","lines":["    "],"id":71}],[{"start":{"row":7,"column":0},"end":{"row":7,"column":4},"action":"remove","lines":["    "],"id":72}],[{"start":{"row":7,"column":0},"end":{"row":7,"column":1},"action":"insert","lines":[" "],"id":73},{"start":{"row":7,"column":1},"end":{"row":7,"column":2},"action":"insert","lines":[" "]}],[{"start":{"row":9,"column":28},"end":{"row":9,"column":37},"action":"remove","lines":["Tu Modelo"],"id":74},{"start":{"row":9,"column":28},"end":{"row":9,"column":62},"action":"insert","lines":["Random Forest (en Test de Twitter)"]}],[{"start":{"row":9,"column":66},"end":{"row":9,"column":77},"action":"remove","lines":["HuggingFace"],"id":75},{"start":{"row":9,"column":66},"end":{"row":9,"column":101},"action":"insert","lines":["DistilBERT (en Validación de SST-2)"]}],[{"start":{"row":9,"column":102},"end":{"row":9,"column":127},"action":"remove","lines":[", \"Diferencia\", \"Conjunto"],"id":76}],[{"start":{"row":9,"column":102},"end":{"row":9,"column":103},"action":"remove","lines":["\""],"id":77}],[{"start":{"row":11,"column":20},"end":{"row":11,"column":26},"action":"remove","lines":["0.9043"],"id":78},{"start":{"row":11,"column":20},"end":{"row":11,"column":25},"action":"insert","lines":["0.748"]}],[{"start":{"row":11,"column":29},"end":{"row":11,"column":35},"action":"remove","lines":["0.9041"],"id":79},{"start":{"row":11,"column":29},"end":{"row":11,"column":33},"action":"insert","lines":["0.79"]}],[{"start":{"row":11,"column":34},"end":{"row":11,"column":53},"action":"remove","lines":[", \"+0.0002\", \"Test\""],"id":80}],[{"start":{"row":12,"column":27},"end":{"row":12,"column":33},"action":"remove","lines":["0.8920"],"id":81},{"start":{"row":12,"column":27},"end":{"row":12,"column":32},"action":"insert","lines":["0.756"]}],[{"start":{"row":12,"column":36},"end":{"row":12,"column":42},"action":"remove","lines":["0.8975"],"id":82},{"start":{"row":12,"column":36},"end":{"row":12,"column":40},"action":"insert","lines":["0.79"]}],[{"start":{"row":12,"column":41},"end":{"row":12,"column":60},"action":"remove","lines":[", \"-0.0055\", \"Test\""],"id":83}],[{"start":{"row":13,"column":24},"end":{"row":13,"column":30},"action":"remove","lines":["0.9012"],"id":84},{"start":{"row":13,"column":24},"end":{"row":13,"column":29},"action":"insert","lines":["0.748"]}],[{"start":{"row":13,"column":33},"end":{"row":13,"column":39},"action":"remove","lines":["0.8892"],"id":85},{"start":{"row":13,"column":33},"end":{"row":13,"column":37},"action":"insert","lines":["0.79"]}],[{"start":{"row":13,"column":38},"end":{"row":13,"column":57},"action":"remove","lines":[", \"+0.0120\", \"Test\""],"id":86}],[{"start":{"row":14,"column":20},"end":{"row":14,"column":26},"action":"remove","lines":["0.8966"],"id":87},{"start":{"row":14,"column":20},"end":{"row":14,"column":25},"action":"insert","lines":["0.748"]}],[{"start":{"row":14,"column":29},"end":{"row":14,"column":35},"action":"remove","lines":["0.8933"],"id":88},{"start":{"row":14,"column":29},"end":{"row":14,"column":33},"action":"insert","lines":["0.79"]}],[{"start":{"row":14,"column":34},"end":{"row":14,"column":54},"action":"remove","lines":[", \"+0.0033\", \"Test\"]"],"id":89},{"start":{"row":14,"column":34},"end":{"row":14,"column":35},"action":"insert","lines":["]"]}],[{"start":{"row":17,"column":18},"end":{"row":17,"column":86},"action":"remove","lines":["Comentario final después de la tabla: conclusiones y próximos pasos."],"id":90},{"start":{"row":17,"column":18},"end":{"row":24,"column":0},"action":"insert","lines":["Como se esperaba, el modelo DistilBERT muestra un rendimiento superior en las métricas clave. Su capacidad para identificar correctamente las clases, reflejada en un recall macro de 0.79, es mayor que la del modelo de Random Forest con 0.748. Esto representa una diferencia de poco más de 4 puntos porcentuales, lo que confirma la ventaja de la arquitectura Transformer, aunque es importante hacer la mención que son bastante parecidos ambos puntajes, lo que se considera un éxito en este aspecto.","","El desempeño del Random Forest se puede justificar en que es un modelo clásico de Machine Learning que opera sobre una representación de Bag of Words, en este caso con datos codificados en bigramas. Aunque efectivo, este enfoque no logra capturar el contexto, la semántica ni el orden de las palabras, lo que termina siendo muy importante para entender la emoción. Por otro lado, se tiene a DistilBERT, el cual es un modelo basado en la arquitectura Transformer. Estos modelos se caracterizan por procesar el texto de manera secuencial y contextual, lo que les permite comprender emociones, sarcasmo y relaciones complejas entre palabras que un modelo como Random Forest no puede capturar.","","Ahora bien, cuando se analiza el origen del entrenamiento, se puede ver que mientras el modelo de Random Forest fue entrenado exclusivamente con datos de Twitter, lo que lo especializa en su informalidad, uso de abreviaturas y estructura de mensajes cortos, el modelo DistilBERT fue fine-tuned en el dataset SST-2, que consiste en reseñas de películas, lo cual implica un lenguaje mucho más formal y estructurado. Y aun así, DistilBERT no solo alcanza al modelo RF, sino que también lo logra superar a pesar de estar en desventaja porque sus datos de entrenamiento no son los mismos. Esto demuestra, por un lado, la capacidad de generalización de las arquitecturas Transformer, y que aprenden patrones lingüísticos profundos que pueden aplicarse de buena manera a dominios para los que no fueron entrenados.","","Por último, se puede definir que es posible construir un modelo de Machine Learning tradicional como Random Forest que, con un preprocesamiento adecuado, alcanza un rendimiento aceptable, con aproximadamente 75% de recall, como una solución base utilizando métodos clásicos. Sin embargo, para el caso de uso propuesto, que es la protección de públicos infantiles, un 75% de recall implica que el 25% de los tuits negativos no serían filtrados, lo cual significa un margen de error bastante grande.",""]}],[{"start":{"row":0,"column":0},"end":{"row":26,"column":0},"action":"remove","lines":["{","  \"text_top\": \"Cuando se realiza la comparativa de Modelos de Análisis de Sentimientos se puede ver que en este caso, ","  el objetivo fue desarrollar un modelo de clasificación de sentimientos para datos de Twitter y comparar su rendimiento con el State-Of-The-Art, ","  como el pipeline sentiment-analysis de Hugging Face, que utiliza el modelo DistilBERT fine-tuned en SST-2. El mejor modelo desarrollado se ","  determina que fue un Random Forest (RF), evaluado principalmente con la métrica de recall debido al caso de uso planteado y acotacion del problema, ","  siendo este el filtrar contenido con connotación negativa para audiencias sensibles, donde es crucial minimizar los falsos negativos. Originalmente ","  planteado como el filtro para un público infantil y evitar su exposición a emociones negativas.","  Al evaluar ambos modelos en sus respectivos conjuntos de prueba, se obtuvieron los siguientes resultados:\t\",","  \"table\": {","    \"headers\": [\"Métrica\", \"Random Forest (en Test de Twitter)\", \"DistilBERT (en Validación de SST-2)\"],","    \"rows\": [","      [\"accuracy\", \"0.748\", \"0.79\"],","      [\"precision_macro\", \"0.756\", \"0.79\"],","      [\"recall_macro\", \"0.748\", \"0.79\"],","      [\"f1_macro\", \"0.748\", \"0.79\"]","    ]","  },","  \"text_bottom\": \"Como se esperaba, el modelo DistilBERT muestra un rendimiento superior en las métricas clave. Su capacidad para identificar correctamente las clases, reflejada en un recall macro de 0.79, es mayor que la del modelo de Random Forest con 0.748. Esto representa una diferencia de poco más de 4 puntos porcentuales, lo que confirma la ventaja de la arquitectura Transformer, aunque es importante hacer la mención que son bastante parecidos ambos puntajes, lo que se considera un éxito en este aspecto.","","El desempeño del Random Forest se puede justificar en que es un modelo clásico de Machine Learning que opera sobre una representación de Bag of Words, en este caso con datos codificados en bigramas. Aunque efectivo, este enfoque no logra capturar el contexto, la semántica ni el orden de las palabras, lo que termina siendo muy importante para entender la emoción. Por otro lado, se tiene a DistilBERT, el cual es un modelo basado en la arquitectura Transformer. Estos modelos se caracterizan por procesar el texto de manera secuencial y contextual, lo que les permite comprender emociones, sarcasmo y relaciones complejas entre palabras que un modelo como Random Forest no puede capturar.","","Ahora bien, cuando se analiza el origen del entrenamiento, se puede ver que mientras el modelo de Random Forest fue entrenado exclusivamente con datos de Twitter, lo que lo especializa en su informalidad, uso de abreviaturas y estructura de mensajes cortos, el modelo DistilBERT fue fine-tuned en el dataset SST-2, que consiste en reseñas de películas, lo cual implica un lenguaje mucho más formal y estructurado. Y aun así, DistilBERT no solo alcanza al modelo RF, sino que también lo logra superar a pesar de estar en desventaja porque sus datos de entrenamiento no son los mismos. Esto demuestra, por un lado, la capacidad de generalización de las arquitecturas Transformer, y que aprenden patrones lingüísticos profundos que pueden aplicarse de buena manera a dominios para los que no fueron entrenados.","","Por último, se puede definir que es posible construir un modelo de Machine Learning tradicional como Random Forest que, con un preprocesamiento adecuado, alcanza un rendimiento aceptable, con aproximadamente 75% de recall, como una solución base utilizando métodos clásicos. Sin embargo, para el caso de uso propuesto, que es la protección de públicos infantiles, un 75% de recall implica que el 25% de los tuits negativos no serían filtrados, lo cual significa un margen de error bastante grande.","\"","}",""],"id":91},{"start":{"row":0,"column":0},"end":{"row":16,"column":1},"action":"insert","lines":["{","  \"text_top\": \"Cuando se realiza la comparativa de Modelos de Análisis de Sentimientos se puede ver que en este caso, \\n  el objetivo fue desarrollar un modelo de clasificación de sentimientos para datos de Twitter y comparar su rendimiento con el State-Of-The-Art, \\n  como el pipeline sentiment-analysis de Hugging Face, que utiliza el modelo DistilBERT fine-tuned en SST-2. El mejor modelo desarrollado se \\n  determina que fue un Random Forest (RF), evaluado principalmente con la métrica de recall debido al caso de uso planteado y acotacion del problema, \\n  siendo este el filtrar contenido con connotación negativa para audiencias sensibles, donde es crucial minimizar los falsos negativos. Originalmente \\n  planteado como el filtro para un público infantil y evitar su exposición a emociones negativas. \\n  Al evaluar ambos modelos en sus respectivos conjuntos de prueba, se obtuvieron los siguientes resultados:\",","  \"table\": {","    \"headers\": [","      \"Métrica\",","      \"Random Forest (en Test de Twitter)\",","      \"DistilBERT (en Validación de SST-2)\"","    ],","    \"rows\": [","      [\"accuracy\", \"0.748\", \"0.79\"],","      [\"precision_macro\", \"0.756\", \"0.79\"],","      [\"recall_macro\", \"0.748\", \"0.79\"],","      [\"f1_macro\", \"0.748\", \"0.79\"]","    ]","  },","  \"text_bottom\": \"Como se esperaba, el modelo DistilBERT muestra un rendimiento superior en las métricas clave. Su capacidad para identificar correctamente las clases, reflejada en un recall macro de 0.79, es mayor que la del modelo de Random Forest con 0.748. Esto representa una diferencia de poco más de 4 puntos porcentuales, lo que confirma la ventaja de la arquitectura Transformer, aunque es importante hacer la mención que son bastante parecidos ambos puntajes, lo que se considera un éxito en este aspecto. \\n\\nEl desempeño del Random Forest se puede justificar en que es un modelo clásico de Machine Learning que opera sobre una representación de Bag of Words, en este caso con datos codificados en bigramas. Aunque efectivo, este enfoque no logra capturar el contexto, la semántica ni el orden de las palabras, lo que termina siendo muy importante para entender la emoción. Por otro lado, se tiene a DistilBERT, el cual es un modelo basado en la arquitectura Transformer. Estos modelos se caracterizan por procesar el texto de manera secuencial y contextual, lo que les permite comprender emociones, sarcasmo y relaciones complejas entre palabras que un modelo como Random Forest no puede capturar. \\n\\nAhora bien, cuando se analiza el origen del entrenamiento, se puede ver que mientras el modelo de Random Forest fue entrenado exclusivamente con datos de Twitter, lo que lo especializa en su informalidad, uso de abreviaturas y estructura de mensajes cortos, el modelo DistilBERT fue fine-tuned en el dataset SST-2, que consiste en reseñas de películas, lo cual implica un lenguaje mucho más formal y estructurado. Y aun así, DistilBERT no solo alcanza al modelo RF, sino que también lo logra superar a pesar de estar en desventaja porque sus datos de entrenamiento no son los mismos. Esto demuestra, por un lado, la capacidad de generalización de las arquitecturas Transformer, y que aprenden patrones lingüísticos profundos que pueden aplicarse de buena manera a dominios para los que no fueron entrenados. \\n\\nPor último, se puede definir que es posible construir un modelo de Machine Learning tradicional como Random Forest que, con un preprocesamiento adecuado, alcanza un rendimiento aceptable, con aproximadamente 75% de recall, como una solución base utilizando métodos clásicos. Sin embargo, para el caso de uso propuesto, que es la protección de públicos infantiles, un 75% de recall implica que el 25% de los tuits negativos no serían filtrados, lo cual significa un margen de error bastante grande.\"","}"]}],[{"start":{"row":0,"column":0},"end":{"row":16,"column":1},"action":"remove","lines":["{","  \"text_top\": \"Cuando se realiza la comparativa de Modelos de Análisis de Sentimientos se puede ver que en este caso, \\n  el objetivo fue desarrollar un modelo de clasificación de sentimientos para datos de Twitter y comparar su rendimiento con el State-Of-The-Art, \\n  como el pipeline sentiment-analysis de Hugging Face, que utiliza el modelo DistilBERT fine-tuned en SST-2. El mejor modelo desarrollado se \\n  determina que fue un Random Forest (RF), evaluado principalmente con la métrica de recall debido al caso de uso planteado y acotacion del problema, \\n  siendo este el filtrar contenido con connotación negativa para audiencias sensibles, donde es crucial minimizar los falsos negativos. Originalmente \\n  planteado como el filtro para un público infantil y evitar su exposición a emociones negativas. \\n  Al evaluar ambos modelos en sus respectivos conjuntos de prueba, se obtuvieron los siguientes resultados:\",","  \"table\": {","    \"headers\": [","      \"Métrica\",","      \"Random Forest (en Test de Twitter)\",","      \"DistilBERT (en Validación de SST-2)\"","    ],","    \"rows\": [","      [\"accuracy\", \"0.748\", \"0.79\"],","      [\"precision_macro\", \"0.756\", \"0.79\"],","      [\"recall_macro\", \"0.748\", \"0.79\"],","      [\"f1_macro\", \"0.748\", \"0.79\"]","    ]","  },","  \"text_bottom\": \"Como se esperaba, el modelo DistilBERT muestra un rendimiento superior en las métricas clave. Su capacidad para identificar correctamente las clases, reflejada en un recall macro de 0.79, es mayor que la del modelo de Random Forest con 0.748. Esto representa una diferencia de poco más de 4 puntos porcentuales, lo que confirma la ventaja de la arquitectura Transformer, aunque es importante hacer la mención que son bastante parecidos ambos puntajes, lo que se considera un éxito en este aspecto. \\n\\nEl desempeño del Random Forest se puede justificar en que es un modelo clásico de Machine Learning que opera sobre una representación de Bag of Words, en este caso con datos codificados en bigramas. Aunque efectivo, este enfoque no logra capturar el contexto, la semántica ni el orden de las palabras, lo que termina siendo muy importante para entender la emoción. Por otro lado, se tiene a DistilBERT, el cual es un modelo basado en la arquitectura Transformer. Estos modelos se caracterizan por procesar el texto de manera secuencial y contextual, lo que les permite comprender emociones, sarcasmo y relaciones complejas entre palabras que un modelo como Random Forest no puede capturar. \\n\\nAhora bien, cuando se analiza el origen del entrenamiento, se puede ver que mientras el modelo de Random Forest fue entrenado exclusivamente con datos de Twitter, lo que lo especializa en su informalidad, uso de abreviaturas y estructura de mensajes cortos, el modelo DistilBERT fue fine-tuned en el dataset SST-2, que consiste en reseñas de películas, lo cual implica un lenguaje mucho más formal y estructurado. Y aun así, DistilBERT no solo alcanza al modelo RF, sino que también lo logra superar a pesar de estar en desventaja porque sus datos de entrenamiento no son los mismos. Esto demuestra, por un lado, la capacidad de generalización de las arquitecturas Transformer, y que aprenden patrones lingüísticos profundos que pueden aplicarse de buena manera a dominios para los que no fueron entrenados. \\n\\nPor último, se puede definir que es posible construir un modelo de Machine Learning tradicional como Random Forest que, con un preprocesamiento adecuado, alcanza un rendimiento aceptable, con aproximadamente 75% de recall, como una solución base utilizando métodos clásicos. Sin embargo, para el caso de uso propuesto, que es la protección de públicos infantiles, un 75% de recall implica que el 25% de los tuits negativos no serían filtrados, lo cual significa un margen de error bastante grande.\"","}"],"id":92},{"start":{"row":0,"column":0},"end":{"row":17,"column":0},"action":"insert","lines":["{","  \"text_top\": \"Cuando se realiza la comparativa de Modelos de Análisis de Sentimientos se puede ver que en este caso, el objetivo fue desarrollar un modelo de clasificación de sentimientos para datos de Twitter y comparar su rendimiento con el State-Of-The-Art, como el pipeline sentiment-analysis de Hugging Face, que utiliza el modelo DistilBERT fine-tuned en SST-2. El mejor modelo desarrollado se determina que fue un Random Forest (RF), evaluado principalmente con la métrica de recall debido al caso de uso planteado y acotacion del problema, siendo este el filtrar contenido con connotación negativa para audiencias sensibles, donde es crucial minimizar los falsos negativos. Originalmente planteado como el filtro para un público infantil y evitar su exposición a emociones negativas. Al evaluar ambos modelos en sus respectivos conjuntos de prueba, se obtuvieron los siguientes resultados:\",","  \"table\": {","    \"headers\": [","      \"Métrica\",","      \"Random Forest (en Test de Twitter)\",","      \"DistilBERT (en Validación de SST-2)\"","    ],","    \"rows\": [","      [\"accuracy\", \"0.748\", \"0.911\"],","      [\"precision_macro\", \"0.756\", \"0.898\"],","      [\"recall_macro\", \"0.748\", \"0.930\"],","      [\"f1_macro\", \"0.748\", \"0.914\"]","    ]","  },","  \"text_bottom\": \"Como se esperaba desde el inicio, el modelo DistilBERT muestra un rendimiento significativamente superior en todas las métricas. Porque su capacidad para identificar correctamente las clases (recall de 0.93) es mucho más alta que la del modelo creado de Random Forest (0.748), lo que representa una diferencia de más de 18 puntos porcentuales. \\n\\nEl desempeño del Random Forest se puede justificar en que es un modelo clásico de Machine Learning que opera sobre una representación de Bag of Words en este caso con datos codificados en Bi gramas. Aunque efectivo, este enfoque no lograr capturar el contexto, la semántica ni el orden de las palabras, lo que termina siendo muy importante para entender la emoción. Por otro lado, se tiene a DistilBERT, el cual es un modelo basado en la arquitectura Transformer. Estos modelos se caracterizan por procesar el texto de manera secuencial y contextual, lo que les permite comprender emociones, sarcasmo y relaciones complejas entre palabras que un modelo como Random Forest no puede capturar. \\n\\nAhora bien, cuando se analiza el origen del entrenamiento, se puede ver que mientras el modelo de Random Forest fue entrenado exclusivamente con datos de Twitter, lo que lo especializa en su informalidad, uso de abreviaturas y estructura de mensajes cortos. El modelo DistilBERT fue fine-tuned en el dataset SST-2, que consiste en reseñas de películas, lo cual implica un lenguaje mucho más formal, estructurado y con un vocabulario distinto al de Twitter. Y aun asi, DistilBERT no solo mejora al modelo RF, sino que lo hace con una gran diferencia. Esto demuestra, por un lado, la capacidad de generalización de las arquitecturas Transformer, y que aprenden patrones lingüísticos profundos que pueden aplicarse de buena manera a dominios para los que no fueron entrenados. \\n\\nPor último, se puede definir que es posible construir un modelo de Machine Learning tradicional como Random Forest que, con un preprocesamiento adecuado, alcanza un rendimiento aceptable (~75% de recall) como una solución base utilizando métodos clásicos. Sin embargo, para el caso de uso propuesto, que es la protección de públicos infantiles, un 75% de recall implica que el 25% de los tuits negativos no serían filtrados, lo cual significa en un margen de error bastante grande.\"","}",""]}],[{"start":{"row":1,"column":15},"end":{"row":1,"column":898},"action":"remove","lines":["Cuando se realiza la comparativa de Modelos de Análisis de Sentimientos se puede ver que en este caso, el objetivo fue desarrollar un modelo de clasificación de sentimientos para datos de Twitter y comparar su rendimiento con el State-Of-The-Art, como el pipeline sentiment-analysis de Hugging Face, que utiliza el modelo DistilBERT fine-tuned en SST-2. El mejor modelo desarrollado se determina que fue un Random Forest (RF), evaluado principalmente con la métrica de recall debido al caso de uso planteado y acotacion del problema, siendo este el filtrar contenido con connotación negativa para audiencias sensibles, donde es crucial minimizar los falsos negativos. Originalmente planteado como el filtro para un público infantil y evitar su exposición a emociones negativas. Al evaluar ambos modelos en sus respectivos conjuntos de prueba, se obtuvieron los siguientes resultados:"],"id":94}],[{"start":{"row":1,"column":15},"end":{"row":1,"column":607},"action":"insert","lines":["Al realizar una comparación de modelos para el análisis de sentimientos en Twitter, se pretendía crear un clasificador y evaluarlo frente al modelo más avanzado disponible (pipeline de Hugging Face que utiliza DistilBERT ajustado en SST-2). El modelo más efectivo desarrollado fue un Random Forest (entrenado con datos de Twitter y representaciones Bag-of-Words en bigramas) y se priorizó la métrica de recall, dado que el propósito del caso de uso es filtrar contenido con connotaciones negativas para audiencias sensibles (como el público infantil) y reducir el número de falsos negativos. "],"id":95}],[{"start":{"row":1,"column":606},"end":{"row":1,"column":607},"action":"remove","lines":[" "],"id":96}],[{"start":{"row":15,"column":18},"end":{"row":15,"column":2321},"action":"remove","lines":["Como se esperaba desde el inicio, el modelo DistilBERT muestra un rendimiento significativamente superior en todas las métricas. Porque su capacidad para identificar correctamente las clases (recall de 0.93) es mucho más alta que la del modelo creado de Random Forest (0.748), lo que representa una diferencia de más de 18 puntos porcentuales. \\n\\nEl desempeño del Random Forest se puede justificar en que es un modelo clásico de Machine Learning que opera sobre una representación de Bag of Words en este caso con datos codificados en Bi gramas. Aunque efectivo, este enfoque no lograr capturar el contexto, la semántica ni el orden de las palabras, lo que termina siendo muy importante para entender la emoción. Por otro lado, se tiene a DistilBERT, el cual es un modelo basado en la arquitectura Transformer. Estos modelos se caracterizan por procesar el texto de manera secuencial y contextual, lo que les permite comprender emociones, sarcasmo y relaciones complejas entre palabras que un modelo como Random Forest no puede capturar. \\n\\nAhora bien, cuando se analiza el origen del entrenamiento, se puede ver que mientras el modelo de Random Forest fue entrenado exclusivamente con datos de Twitter, lo que lo especializa en su informalidad, uso de abreviaturas y estructura de mensajes cortos. El modelo DistilBERT fue fine-tuned en el dataset SST-2, que consiste en reseñas de películas, lo cual implica un lenguaje mucho más formal, estructurado y con un vocabulario distinto al de Twitter. Y aun asi, DistilBERT no solo mejora al modelo RF, sino que lo hace con una gran diferencia. Esto demuestra, por un lado, la capacidad de generalización de las arquitecturas Transformer, y que aprenden patrones lingüísticos profundos que pueden aplicarse de buena manera a dominios para los que no fueron entrenados. \\n\\nPor último, se puede definir que es posible construir un modelo de Machine Learning tradicional como Random Forest que, con un preprocesamiento adecuado, alcanza un rendimiento aceptable (~75% de recall) como una solución base utilizando métodos clásicos. Sin embargo, para el caso de uso propuesto, que es la protección de públicos infantiles, un 75% de recall implica que el 25% de los tuits negativos no serían filtrados, lo cual significa en un margen de error bastante grande.\""],"id":97}],[{"start":{"row":15,"column":18},"end":{"row":15,"column":731},"action":"insert","lines":["Durante las evaluaciones, DistilBERT superó notablemente al Random Forest (recall de 0. 93 frente a 0. 75 más de 18 puntos porcentuales), lo cual es atribuible a la capacidad de los Transformers para captar contexto, semántica, sarcasmo y relaciones entre palabras, en contraste con el Random Forest que trabaja con representaciones que no mantienen el orden ni el contexto; Aunque un Random Forest bien procesado puede lograr un recall aceptable (~75%) y ser considerado como una solución base, para la protección de públicos infantiles, ese nivel significa que aproximadamente el 25% de los tweets negativos no serían filtrados, lo cual representa un margen de error demasiado alto para el objetivo establecido."],"id":98}],[{"start":{"row":15,"column":731},"end":{"row":15,"column":732},"action":"insert","lines":["\""],"id":99}],[{"start":{"row":9,"column":22},"end":{"row":9,"column":25},"action":"remove","lines":["748"],"id":100},{"start":{"row":9,"column":22},"end":{"row":9,"column":26},"action":"insert","lines":["7834"]}],[{"start":{"row":10,"column":29},"end":{"row":10,"column":32},"action":"remove","lines":["756"],"id":101},{"start":{"row":10,"column":29},"end":{"row":10,"column":33},"action":"insert","lines":["7590"]}],[{"start":{"row":11,"column":26},"end":{"row":11,"column":29},"action":"remove","lines":["748"],"id":102},{"start":{"row":11,"column":26},"end":{"row":11,"column":30},"action":"insert","lines":["8304"]}],[{"start":{"row":12,"column":22},"end":{"row":12,"column":25},"action":"remove","lines":["748"],"id":103},{"start":{"row":12,"column":22},"end":{"row":12,"column":26},"action":"insert","lines":["7931"]}],[{"start":{"row":9,"column":32},"end":{"row":9,"column":35},"action":"remove","lines":["911"],"id":104},{"start":{"row":9,"column":32},"end":{"row":9,"column":36},"action":"insert","lines":["7203"]}],[{"start":{"row":10,"column":39},"end":{"row":10,"column":42},"action":"remove","lines":["898"],"id":105},{"start":{"row":10,"column":39},"end":{"row":10,"column":43},"action":"insert","lines":["7252"]}],[{"start":{"row":11,"column":36},"end":{"row":11,"column":39},"action":"remove","lines":["930"],"id":106},{"start":{"row":11,"column":36},"end":{"row":11,"column":40},"action":"insert","lines":["7203"]}],[{"start":{"row":12,"column":32},"end":{"row":12,"column":35},"action":"remove","lines":["914"],"id":107},{"start":{"row":12,"column":32},"end":{"row":12,"column":36},"action":"insert","lines":["7188"]}],[{"start":{"row":5,"column":7},"end":{"row":5,"column":20},"action":"remove","lines":["Random Forest"],"id":108},{"start":{"row":5,"column":7},"end":{"row":5,"column":16},"action":"insert","lines":["LightGBM "]}],[{"start":{"row":5,"column":15},"end":{"row":5,"column":16},"action":"remove","lines":[" "],"id":109}],[{"start":{"row":15,"column":18},"end":{"row":15,"column":732},"action":"remove","lines":["Durante las evaluaciones, DistilBERT superó notablemente al Random Forest (recall de 0. 93 frente a 0. 75 más de 18 puntos porcentuales), lo cual es atribuible a la capacidad de los Transformers para captar contexto, semántica, sarcasmo y relaciones entre palabras, en contraste con el Random Forest que trabaja con representaciones que no mantienen el orden ni el contexto; Aunque un Random Forest bien procesado puede lograr un recall aceptable (~75%) y ser considerado como una solución base, para la protección de públicos infantiles, ese nivel significa que aproximadamente el 25% de los tweets negativos no serían filtrados, lo cual representa un margen de error demasiado alto para el objetivo establecido.\""],"id":110},{"start":{"row":15,"column":18},"end":{"row":15,"column":404},"action":"insert","lines":["Contrario a lo que se podría esperar, el modelo LightGBM desarrollado internamente muestra un rendimiento significativamente superior en todas las métricas clave para este problema. Su capacidad para identificar correctamente las clases (recall de 0.83) es mucho más alta que la del modelo DistilBERT sobre los mismos datos (0.72), lo que representa una diferencia de más de 11 décimas."]}],[{"start":{"row":15,"column":404},"end":{"row":15,"column":405},"action":"insert","lines":["\""],"id":111}],[{"start":{"row":15,"column":405},"end":{"row":16,"column":0},"action":"insert","lines":["",""],"id":112},{"start":{"row":16,"column":0},"end":{"row":16,"column":2},"action":"insert","lines":["  "]}],[{"start":{"row":16,"column":0},"end":{"row":16,"column":2},"action":"remove","lines":["  "],"id":113},{"start":{"row":15,"column":405},"end":{"row":16,"column":0},"action":"remove","lines":["",""]}],[{"start":{"row":15,"column":405},"end":{"row":15,"column":406},"action":"insert","lines":[","],"id":114}],[{"start":{"row":15,"column":406},"end":{"row":16,"column":0},"action":"insert","lines":["",""],"id":115},{"start":{"row":16,"column":0},"end":{"row":16,"column":2},"action":"insert","lines":["  "]}],[{"start":{"row":16,"column":2},"end":{"row":16,"column":404},"action":"insert","lines":["\"text_bottom\": \"Contrario a lo que se podría esperar, el modelo LightGBM desarrollado internamente muestra un rendimiento significativamente superior en todas las métricas clave para este problema. Su capacidad para identificar correctamente las clases (recall de 0.83) es mucho más alta que la del modelo DistilBERT sobre los mismos datos (0.72), lo que representa una diferencia de más de 11 décimas."],"id":116}],[{"start":{"row":16,"column":404},"end":{"row":16,"column":405},"action":"insert","lines":["\""],"id":117}],[{"start":{"row":1,"column":15},"end":{"row":1,"column":607},"action":"remove","lines":["Al realizar una comparación de modelos para el análisis de sentimientos en Twitter, se pretendía crear un clasificador y evaluarlo frente al modelo más avanzado disponible (pipeline de Hugging Face que utiliza DistilBERT ajustado en SST-2). El modelo más efectivo desarrollado fue un Random Forest (entrenado con datos de Twitter y representaciones Bag-of-Words en bigramas) y se priorizó la métrica de recall, dado que el propósito del caso de uso es filtrar contenido con connotaciones negativas para audiencias sensibles (como el público infantil) y reducir el número de falsos negativos.\""],"id":118},{"start":{"row":1,"column":15},"end":{"row":1,"column":827},"action":"insert","lines":["Cuando se realiza la comparativa de Modelos de Análisis de Sentimientos se puede ver que en este caso, el objetivo fue desarrollar un modelo de clasificación de sentimientos para datos de Twitter y comparar su rendimiento con el State-Of-The-Art, como el pipeline sentiment-analysis de Hugging Face, que utiliza el modelo DistilBERT fine-tuned en SST-2. El mejor modelo desarrollado se determina que fue un LightGBM Pipeline 3 con una codificación de bigramas, que se puede considerar como la versión mejorada del random forest para esta tarea. El modelo fue evaluado principalmente con la métrica de recall_macro (0.83) debido al caso de uso planteado y acotación del problema, siendo este el filtrar contenido con connotación negativa para audiencias sensibles, donde es crucial minimizar los falsos negativos."]}],[{"start":{"row":1,"column":827},"end":{"row":1,"column":828},"action":"insert","lines":["\""],"id":119}],[{"start":{"row":16,"column":0},"end":{"row":17,"column":0},"action":"remove","lines":["  \"text_bottom\": \"Contrario a lo que se podría esperar, el modelo LightGBM desarrollado internamente muestra un rendimiento significativamente superior en todas las métricas clave para este problema. Su capacidad para identificar correctamente las clases (recall de 0.83) es mucho más alta que la del modelo DistilBERT sobre los mismos datos (0.72), lo que representa una diferencia de más de 11 décimas.\"",""],"id":120}],[{"start":{"row":15,"column":405},"end":{"row":15,"column":406},"action":"remove","lines":[","],"id":121}],[{"start":{"row":0,"column":0},"end":{"row":17,"column":0},"action":"remove","lines":["{","  \"text_top\": \"Cuando se realiza la comparativa de Modelos de Análisis de Sentimientos se puede ver que en este caso, el objetivo fue desarrollar un modelo de clasificación de sentimientos para datos de Twitter y comparar su rendimiento con el State-Of-The-Art, como el pipeline sentiment-analysis de Hugging Face, que utiliza el modelo DistilBERT fine-tuned en SST-2. El mejor modelo desarrollado se determina que fue un LightGBM Pipeline 3 con una codificación de bigramas, que se puede considerar como la versión mejorada del random forest para esta tarea. El modelo fue evaluado principalmente con la métrica de recall_macro (0.83) debido al caso de uso planteado y acotación del problema, siendo este el filtrar contenido con connotación negativa para audiencias sensibles, donde es crucial minimizar los falsos negativos.\",","  \"table\": {","    \"headers\": [","      \"Métrica\",","      \"LightGBM (en Test de Twitter)\",","      \"DistilBERT (en Validación de SST-2)\"","    ],","    \"rows\": [","      [\"accuracy\", \"0.7834\", \"0.7203\"],","      [\"precision_macro\", \"0.7590\", \"0.7252\"],","      [\"recall_macro\", \"0.8304\", \"0.7203\"],","      [\"f1_macro\", \"0.7931\", \"0.7188\"]","    ]","  },","  \"text_bottom\": \"Contrario a lo que se podría esperar, el modelo LightGBM desarrollado internamente muestra un rendimiento significativamente superior en todas las métricas clave para este problema. Su capacidad para identificar correctamente las clases (recall de 0.83) es mucho más alta que la del modelo DistilBERT sobre los mismos datos (0.72), lo que representa una diferencia de más de 11 décimas.\"","}",""],"id":122},{"start":{"row":0,"column":0},"end":{"row":16,"column":1},"action":"insert","lines":["{","  \"text_top\": \"Al realizar la comparación de modelos de análisis de sentimientos, el objetivo principal fue desarrollar un clasificador para datos de Twitter y evaluar su rendimiento frente a un modelo de referencia State of the Art: el pipeline sentiment-analysis de Hugging Face, basado en DistilBERT ajustado sobre SST-2.\\n\\nEntre los modelos implementados, el mejor desempeño lo obtuvo el LightGBM Pipeline 3, entrenado con una codificación de bigramas. Este modelo puede considerarse una versión más eficiente y precisa que el Random Forest en esta tarea específica.\\n\\nLa métrica principal de evaluación fue el recall macro (0.83), seleccionada debido al caso de uso planteado: la detección de contenido con connotación negativa dirigido a audiencias sensibles. En este contexto, minimizar los falsos negativos es fundamental para evitar que mensajes nocivos pasen inadvertidos.\\n\\nFinalmente, ambos modelos fueron evaluados sobre el mismo conjunto de prueba (X_test), obteniéndose los siguientes resultados:\",","  \"table\": {","    \"headers\": [","      \"Métrica\",","      \"LightGBM (en Test de Twitter)\",","      \"DistilBERT (en Validación de SST-2)\"","    ],","    \"rows\": [","      [\"accuracy\", \"0.7834\", \"0.7203\"],","      [\"precision_macro\", \"0.7590\", \"0.7252\"],","      [\"recall_macro\", \"0.8304\", \"0.7203\"],","      [\"f1_macro\", \"0.7931\", \"0.7188\"]","    ]","  },","  \"text_bottom\": \"Contrario a lo que podría anticiparse, el modelo LightGBM desarrollado internamente alcanzó un rendimiento significativamente superior en todas las métricas clave. En particular, su capacidad de identificar correctamente las clases, medida mediante el recall (0.83), supera ampliamente a la obtenida por DistilBERT sobre los mismos datos (0.72), lo que representa una diferencia superior a once décimas.\\n\\nEste resultado se explica por la naturaleza de los datos y el ajuste del modelo. Aunque LightGBM es un enfoque clásico basado en representaciones Bag of Words —en este caso con bigramas—, fue entrenado y optimizado específicamente para el lenguaje y la estructura propios de Twitter. Por contraste, DistilBERT, como modelo basado en la arquitectura Transformer, posee gran potencia, pero su desempeño depende fuertemente de la similitud entre los datos de entrenamiento y los de inferencia.\\n\\nAquí se evidencia un problema recurrente en Machine Learning: la discrepancia entre dominios de datos. Mientras que LightGBM fue entrenado exclusivamente con mensajes de Twitter, DistilBERT fue ajustado sobre el conjunto SST-2 (reseñas de películas), caracterizado por un lenguaje más formal y estructurado. Al enfrentarse a la informalidad, abreviaturas y particularidades semánticas de Twitter, DistilBERT experimentó una disminución significativa en su rendimiento. Esto demuestra que un modelo especializado, incluso si es tecnológicamente más sencillo, puede superar a uno más avanzado cuando este último no está adaptado al dominio específico.\\n\\nEn conclusión, es posible desarrollar un modelo tradicional como LightGBM que, con un preprocesamiento adecuado, logre un rendimiento competitivo, alcanzando un recall del 83%. Sin embargo, en el contexto del caso de uso planteado —la protección de audiencias infantiles—, ese valor implica que un 17% de los tuits negativos no serían filtrados. Si bien esto representa una mejora considerable respecto a análisis anteriores, aún constituye un margen de error relevante que debe tomarse en cuenta.\"","}"]}]]},"ace":{"folds":[],"scrolltop":0,"scrollleft":0,"selection":{"start":{"row":0,"column":0},"end":{"row":16,"column":1},"isBackwards":false},"options":{"guessTabSize":true,"useWrapMode":false,"wrapToView":true},"firstLineState":{"row":578,"mode":"ace/mode/json"}},"timestamp":1758046217615,"hash":"f79f6bdcb3f4a934a4d018d1e61218af2df8902f"}